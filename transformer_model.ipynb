{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchinfo import summary  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('X_train.npy')\n",
    "X_val = np.load('X_val.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "y_val = np.load('y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1587, 100, 9)\n",
      "X_val.shape: (397, 100, 9)\n",
      "y_train.shape: (1587,)\n",
      "y_val.shape: (397,)\n",
      "y_train class distribution: [1505   82]\n",
      "y_val class distribution: [376  21]\n"
     ]
    }
   ],
   "source": [
    "#Print Dataset for Verification\n",
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"X_val.shape: {X_val.shape}\")\n",
    "print(f\"y_train.shape: {y_train.shape}\")\n",
    "print(f\"y_val.shape: {y_val.shape}\")\n",
    "#Print Class Distribution\n",
    "print(f\"y_train class distribution: {np.bincount(y_train.astype(int))}\")\n",
    "print(f\"y_val class distribution: {np.bincount(y_val.astype(int))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Class Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted class weights: {0: 0.5272425249169436, 1: 7.741463414634147}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Adjust weight for positive class (reduce by 20%)\n",
    "class_weight_dict[1] = class_weight_dict[1] * 0.8\n",
    "print(\"Adjusted class weights:\", class_weight_dict)\n",
    "\n",
    "# Convert class weights to tensor\n",
    "class_weights_tensor = torch.FloatTensor([class_weight_dict[0], class_weight_dict[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset + DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values if hasattr(y, 'values') else y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "train_ds = TabularDataset(X_train, y_train)\n",
    "val_ds = TabularDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_weights_tensor = class_weights_tensor.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "SAINTTransformer                              [64, 1]                   6,400\n",
       "├─Linear: 1-1                                 [64, 100, 64]             640\n",
       "├─TransformerEncoder: 1-2                     [64, 100, 64]             --\n",
       "│    └─ModuleList: 2-1                        --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-1      [64, 100, 64]             281,152\n",
       "│    │    └─TransformerEncoderLayer: 3-2      [64, 100, 64]             281,152\n",
       "│    │    └─TransformerEncoderLayer: 3-3      [64, 100, 64]             281,152\n",
       "├─Sequential: 1-3                             [64, 100, 1]              --\n",
       "│    └─Linear: 2-2                            [64, 100, 128]            8,320\n",
       "│    └─Tanh: 2-3                              [64, 100, 128]            --\n",
       "│    └─Linear: 2-4                            [64, 100, 1]              129\n",
       "├─Linear: 1-4                                 [64, 32]                  2,080\n",
       "├─ReLU: 1-5                                   [64, 32]                  --\n",
       "├─Linear: 1-6                                 [64, 1]                   33\n",
       "===============================================================================================\n",
       "Total params: 861,058\n",
       "Trainable params: 861,058\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 51.50\n",
       "===============================================================================================\n",
       "Input size (MB): 0.23\n",
       "Forward/backward pass size (MB): 353.96\n",
       "Params size (MB): 3.22\n",
       "Estimated Total Size (MB): 357.41\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SAINTTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=64, num_heads=4, num_layers=3, dropout=0.1, max_seq_len=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "\n",
    "        # Learnable positional encoding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_seq_len, embedding_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, \n",
    "            nhead=num_heads, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Attention pooling layer\n",
    "        self.attention_pool = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "        self.hidden = nn.Linear(embedding_dim, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.classifier = nn.Linear(32, 1)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        B, T, _ = x.size()\n",
    "\n",
    "        x = self.embedding(x)  # (B, T, embedding_dim)\n",
    "\n",
    "        # Add positional encoding (trim to T)\n",
    "        x = x + self.pos_embedding[:, :T, :]\n",
    "\n",
    "        x = self.transformer(x)  # (B, T, embedding_dim)\n",
    "\n",
    "        # Attention pooling\n",
    "        attn_weights = self.attention_pool(x)  # (B, T, 1)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)  # (B, T, 1)\n",
    "        x = torch.sum(x * attn_weights, dim=1)  # (B, embedding_dim)\n",
    "\n",
    "        x = self.relu(self.hidden(x))  # (B, 32)\n",
    "        logits = self.classifier(x)    # (B, 1)\n",
    "        return logits\n",
    "    \n",
    "n_features = X_train.shape[2]  # Get number of features from the data\n",
    "model = SAINTTransformer(input_dim=n_features).to(device)\n",
    "summary(model, input_size=(64, 100, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criterion (Loss Function) + Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor[1])  # Use pos_weight for binary classification\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epochs & Record Best Train & Val Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "best_val_auc = 0\n",
    "best_val_cm = None\n",
    "best_train_auc = 0\n",
    "best_train_cm = None\n",
    "best_train_prec = 0\n",
    "best_train_rec = 0\n",
    "best_train_f1 = 0\n",
    "\n",
    "history = {\n",
    "    'train_prec': [], 'val_prec': [],\n",
    "    'train_rec': [], 'val_rec': [],\n",
    "    'train_f1': [], 'val_f1': [],\n",
    "    'train_auc': [], 'val_auc': [],\n",
    "}\n",
    "\n",
    "loss = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "}\n",
    "\n",
    "acc = {\n",
    "    'train_acc': [], 'val_acc':[],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training Process & Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, auc, roc_curve, precision_recall_curve, average_precision_score\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs.squeeze(1), y_batch.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        probs = torch.sigmoid(outputs).squeeze(dim=1).detach().cpu().numpy()\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "        all_probs.extend(probs)\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_auc = roc_auc_score(all_targets, all_probs)\n",
    "    epoch_prec = precision_score(all_targets, all_preds)\n",
    "    epoch_rec = recall_score(all_targets, all_preds)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds)\n",
    "    epoch_acc = accuracy_score(all_targets, all_preds)\n",
    "    epoch_cm = confusion_matrix(all_targets, all_preds)\n",
    "    return epoch_loss, epoch_auc, epoch_acc, epoch_prec, epoch_rec, epoch_f1, epoch_cm\n",
    "\n",
    "def eval_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs.squeeze(1), y_batch.float())\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(outputs).squeeze(dim=1).detach().cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_auc = roc_auc_score(all_targets, all_probs)\n",
    "    epoch_prec = precision_score(all_targets, all_preds)\n",
    "    epoch_rec = recall_score(all_targets, all_preds)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds)\n",
    "    epoch_acc = accuracy_score(all_targets, all_preds)\n",
    "    epoch_cm = confusion_matrix(all_targets, all_preds)\n",
    "    return epoch_loss, epoch_auc, epoch_acc, epoch_prec, epoch_rec, epoch_f1, epoch_cm, all_targets, all_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 0.001\n",
      "Epoch 1/20 - Train Loss: 0.6893, AUC: 0.7875, Acc: 0.8198, Prec: 0.1304, Rec: 0.4390, F1: 0.2011\n",
      "Val Loss: 0.5893, AUC: 0.8499, Acc: 0.7355, Prec: 0.1557, Rec: 0.9048, F1: 0.2657\n",
      "Current LR: 0.001\n",
      "Epoch 2/20 - Train Loss: 0.6021, AUC: 0.8196, Acc: 0.7839, Prec: 0.1645, Rec: 0.7805, F1: 0.2718\n",
      "Val Loss: 0.6280, AUC: 0.7898, Acc: 0.7531, Prec: 0.1532, Rec: 0.8095, F1: 0.2576\n",
      "Current LR: 0.001\n",
      "Epoch 3/20 - Train Loss: 0.5977, AUC: 0.8196, Acc: 0.7687, Prec: 0.1550, Rec: 0.7805, F1: 0.2586\n",
      "Val Loss: 0.6353, AUC: 0.7950, Acc: 0.8060, Prec: 0.1585, Rec: 0.6190, F1: 0.2524\n",
      "Current LR: 0.001\n",
      "Epoch 4/20 - Train Loss: 0.5500, AUC: 0.8525, Acc: 0.8173, Prec: 0.1941, Rec: 0.8049, F1: 0.3128\n",
      "Val Loss: 0.5642, AUC: 0.8345, Acc: 0.7708, Prec: 0.1759, Rec: 0.9048, F1: 0.2946\n",
      "Current LR: 0.001\n",
      "Epoch 5/20 - Train Loss: 0.5541, AUC: 0.8278, Acc: 0.7845, Prec: 0.1734, Rec: 0.8415, F1: 0.2875\n",
      "Val Loss: 0.5739, AUC: 0.8267, Acc: 0.7607, Prec: 0.1696, Rec: 0.9048, F1: 0.2857\n",
      "Current LR: 0.001\n",
      "Epoch 6/20 - Train Loss: 0.5353, AUC: 0.8353, Acc: 0.7719, Prec: 0.1729, Rec: 0.9024, F1: 0.2902\n",
      "Val Loss: 0.5520, AUC: 0.8526, Acc: 0.8086, Prec: 0.1978, Rec: 0.8571, F1: 0.3214\n",
      "Current LR: 0.001\n",
      "Epoch 7/20 - Train Loss: 0.5368, AUC: 0.8276, Acc: 0.7984, Prec: 0.1835, Rec: 0.8415, F1: 0.3013\n",
      "Val Loss: 0.5527, AUC: 0.8543, Acc: 0.8060, Prec: 0.1957, Rec: 0.8571, F1: 0.3186\n",
      "Current LR: 0.001\n",
      "Epoch 8/20 - Train Loss: 0.5469, AUC: 0.8419, Acc: 0.8040, Prec: 0.1828, Rec: 0.8049, F1: 0.2980\n",
      "Val Loss: 0.5817, AUC: 0.8079, Acc: 0.7809, Prec: 0.1700, Rec: 0.8095, F1: 0.2810\n",
      "Current LR: 0.001\n",
      "Epoch 9/20 - Train Loss: 0.5697, AUC: 0.8266, Acc: 0.8059, Prec: 0.1753, Rec: 0.7439, F1: 0.2837\n",
      "Val Loss: 0.5963, AUC: 0.8164, Acc: 0.7909, Prec: 0.1630, Rec: 0.7143, F1: 0.2655\n",
      "Current LR: 0.001\n",
      "Epoch 10/20 - Train Loss: 0.5888, AUC: 0.8179, Acc: 0.7927, Prec: 0.1598, Rec: 0.7073, F1: 0.2607\n",
      "Val Loss: 0.6323, AUC: 0.8234, Acc: 0.8791, Prec: 0.1860, Rec: 0.3810, F1: 0.2500\n",
      "Current LR: 0.001\n",
      "Epoch 11/20 - Train Loss: 0.6076, AUC: 0.8261, Acc: 0.8242, Prec: 0.1792, Rec: 0.6707, F1: 0.2828\n",
      "Val Loss: 0.6650, AUC: 0.7853, Acc: 0.7683, Prec: 0.1414, Rec: 0.6667, F1: 0.2333\n",
      "Current LR: 0.001\n",
      "Epoch 12/20 - Train Loss: 0.6250, AUC: 0.8030, Acc: 0.7675, Prec: 0.1509, Rec: 0.7561, F1: 0.2515\n",
      "Val Loss: 0.6188, AUC: 0.8018, Acc: 0.8741, Prec: 0.1778, Rec: 0.3810, F1: 0.2424\n",
      "Current LR: 0.001\n",
      "Epoch 13/20 - Train Loss: 0.5903, AUC: 0.8285, Acc: 0.8122, Prec: 0.1727, Rec: 0.6951, F1: 0.2767\n",
      "Val Loss: 0.6128, AUC: 0.8076, Acc: 0.8262, Prec: 0.1667, Rec: 0.5714, F1: 0.2581\n",
      "Current LR: 0.001\n",
      "Epoch 14/20 - Train Loss: 0.5754, AUC: 0.8257, Acc: 0.7782, Prec: 0.1625, Rec: 0.7927, F1: 0.2697\n",
      "Val Loss: 0.5584, AUC: 0.8466, Acc: 0.8136, Prec: 0.2022, Rec: 0.8571, F1: 0.3273\n",
      "Current LR: 0.001\n",
      "Epoch 15/20 - Train Loss: 0.5444, AUC: 0.8244, Acc: 0.8116, Prec: 0.1873, Rec: 0.7927, F1: 0.3030\n",
      "Val Loss: 0.5887, AUC: 0.8057, Acc: 0.8161, Prec: 0.1667, Rec: 0.6190, F1: 0.2626\n",
      "Current LR: 0.001\n",
      "Epoch 16/20 - Train Loss: 0.5463, AUC: 0.8267, Acc: 0.7952, Prec: 0.1743, Rec: 0.7927, F1: 0.2857\n",
      "Val Loss: 0.5670, AUC: 0.8601, Acc: 0.7708, Prec: 0.1759, Rec: 0.9048, F1: 0.2946\n",
      "Current LR: 0.001\n",
      "Epoch 17/20 - Train Loss: 0.5388, AUC: 0.8317, Acc: 0.7858, Prec: 0.1791, Rec: 0.8780, F1: 0.2975\n",
      "Val Loss: 0.5606, AUC: 0.8424, Acc: 0.7909, Prec: 0.1837, Rec: 0.8571, F1: 0.3025\n",
      "Current LR: 0.001\n",
      "Epoch 18/20 - Train Loss: 0.5202, AUC: 0.8249, Acc: 0.7933, Prec: 0.1862, Rec: 0.8902, F1: 0.3080\n",
      "Val Loss: 0.5504, AUC: 0.8525, Acc: 0.8111, Prec: 0.2000, Rec: 0.8571, F1: 0.3243\n",
      "Current LR: 0.001\n",
      "Epoch 19/20 - Train Loss: 0.5290, AUC: 0.8333, Acc: 0.7839, Prec: 0.1809, Rec: 0.9024, F1: 0.3014\n",
      "Val Loss: 0.5533, AUC: 0.8461, Acc: 0.7783, Prec: 0.1810, Rec: 0.9048, F1: 0.3016\n",
      "Current LR: 0.001\n",
      "Epoch 20/20 - Train Loss: 0.5313, AUC: 0.8222, Acc: 0.7763, Prec: 0.1742, Rec: 0.8902, F1: 0.2914\n",
      "Val Loss: 0.5522, AUC: 0.8487, Acc: 0.8136, Prec: 0.2022, Rec: 0.8571, F1: 0.3273\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "        train_loss, train_auc, train_acc, train_prec, train_rec, train_f1, train_cm = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_auc, val_acc, val_prec, val_rec, val_f1, val_cm, all_targets, all_probs = eval_one_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Current LR: {optimizer.param_groups[0]['lr']}\")\n",
    "        history['train_prec'].append(train_prec)\n",
    "        history['val_prec'].append(val_prec)\n",
    "\n",
    "        history['train_rec'].append(train_rec)\n",
    "        history['val_rec'].append(val_rec)\n",
    "\n",
    "        history['train_f1'].append(train_f1)\n",
    "        history['val_f1'].append(val_f1)\n",
    "\n",
    "        history['train_auc'].append(train_auc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "\n",
    "        loss['train_loss'].append(train_loss)\n",
    "        loss['val_loss'].append(val_loss)\n",
    "\n",
    "        acc['train_acc'].append(train_acc)\n",
    "        acc['val_acc'].append(val_acc)\n",
    "\n",
    "        # Track best validation\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_val_cm = val_cm\n",
    "            best_val_prec = val_prec\n",
    "            best_val_rec = val_rec\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_saint_model.pth')\n",
    "\n",
    "        # Track best training\n",
    "        if train_auc > best_train_auc:\n",
    "            best_train_auc = train_auc\n",
    "            best_train_cm = train_cm\n",
    "            best_train_prec = train_prec\n",
    "            best_train_rec = train_rec\n",
    "            best_train_f1 = train_f1\n",
    "            best_train_acc = train_acc\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, AUC: {train_auc:.4f}, Acc: {train_acc:.4f}, Prec: {train_prec:.4f}, Rec: {train_rec:.4f}, F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, Acc: {val_acc:.4f}, Prec: {val_prec:.4f}, Rec: {val_rec:.4f}, F1: {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Metrics (AUC-ROC, Precision, Recall, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(history, filename):\n",
    "    # Get final epoch metrics\n",
    "    metrics = ['Precision', 'Recall', 'F1-score', 'AUC']\n",
    "    train_scores = [\n",
    "        history['train_prec'][-1],\n",
    "        history['train_rec'][-1],\n",
    "        history['train_f1'][-1],\n",
    "        history['train_auc'][-1]\n",
    "    ]\n",
    "    val_scores = [\n",
    "        history['val_prec'][-1],\n",
    "        history['val_rec'][-1],\n",
    "        history['val_f1'][-1],\n",
    "        history['val_auc'][-1]\n",
    "    ]\n",
    "\n",
    "    x = np.arange(len(metrics))  # Metric categories\n",
    "    width = 0.35  # Bar width\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(x - width/2, train_scores, width, label='Train', color='skyblue')\n",
    "    plt.bar(x + width/2, val_scores, width, label='Validation', color='lightcoral')\n",
    "\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.title('Final Training vs Validation Metrics')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.legend()\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "plot_metrics(history, 'training_validation_metrics.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Loss & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_and_accuracy(loss_history, acc_history, loss_filename, acc_filename):\n",
    "    epochs = range(1, len(loss_history['train_loss']) + 1)\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(epochs, loss_history['train_loss'], 'b-', label='Train Loss')\n",
    "    plt.plot(epochs, loss_history['val_loss'], 'r--', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(loss_filename)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(epochs, acc_history['train_acc'], 'g-', label='Train Accuracy')\n",
    "    plt.plot(epochs, acc_history['val_acc'], 'm--', label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(acc_filename)\n",
    "    plt.close()\n",
    "    \n",
    "plot_loss_and_accuracy(loss, acc, 'training_validation_loss.png', 'training_validation_acc.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_scores, filename):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate (Recall)')\n",
    "    plt.title('AUC-ROC Curve (Test Set)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "plot_roc_curve(all_targets, all_probs, 'val_roc_curve.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(y_true, y_scores, filename):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "    avg_precision = average_precision_score(y_true, y_scores)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (AP = {avg_precision:.4f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve (Validation)')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    \n",
    "plot_precision_recall_curve(all_targets, all_probs, 'val_precision_recall_curve.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Best Evaluation Metrics (Training & Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Training AUC: 0.8525\n",
      "Best Training Precision: 0.1941\n",
      "Best Training Recall: 0.8049\n",
      "Best Training F1-score: 0.3128\n",
      "Best Training Confusion Matrix:\n",
      "[[1231  274]\n",
      " [  16   66]]\n",
      "\n",
      "Best Validation AUC: 0.8601\n",
      "Best Validation Precision: 0.1759\n",
      "Best Validation Recall: 0.9048\n",
      "Best Validation F1-score: 0.2946\n",
      "Best Validation Confusion Matrix:\n",
      "[[287  89]\n",
      " [  2  19]]\n"
     ]
    }
   ],
   "source": [
    "def plot_and_save_cm(cm, title, filename):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "# Final metric printouts\n",
    "# print(f\"\\nBest Training Accuracy: {best_train_acc:.4f}\")\n",
    "print(f\"\\nBest Training AUC: {best_train_auc:.4f}\")\n",
    "print(f\"Best Training Precision: {best_train_prec:.4f}\")\n",
    "print(f\"Best Training Recall: {best_train_rec:.4f}\")\n",
    "print(f\"Best Training F1-score: {best_train_f1:.4f}\")\n",
    "print(\"Best Training Confusion Matrix:\")\n",
    "print(best_train_cm)\n",
    "plot_and_save_cm(best_train_cm, 'Best Training Confusion Matrix', 'best_train_cm.png')\n",
    "\n",
    "# print(f\"\\nBest Validation Accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"\\nBest Validation AUC: {best_val_auc:.4f}\")\n",
    "print(f\"Best Validation Precision: {best_val_prec:.4f}\")\n",
    "print(f\"Best Validation Recall: {best_val_rec:.4f}\")\n",
    "print(f\"Best Validation F1-score: {best_val_f1:.4f}\")\n",
    "print(\"Best Validation Confusion Matrix:\")\n",
    "print(best_val_cm)\n",
    "plot_and_save_cm(best_val_cm, 'Best Validation Confusion Matrix', 'best_val_cm.png')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
